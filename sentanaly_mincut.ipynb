{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentanaly_mincut.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1pFx35XpB6so-jljvYWBjElpX5jFfZzo0",
      "authorship_tag": "ABX9TyO3YnhK9B5TxkFPvQ9G+2bn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagannath311/DataScience-/blob/master/sentanaly_mincut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLb6HDXoLWHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##the required packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import collections\n",
        "##loading files path from drive\n",
        "files=os.listdir(r'/content/drive/My Drive/txt_sentoken/pos')\n",
        "files1=os.listdir(r'/content/drive/My Drive/txt_sentoken/neg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jV33DQRmrgU",
        "colab_type": "text"
      },
      "source": [
        "# Building the subjectivitydescriptor with files from http://www.cs.cornell.edu/people/pabo/movie-review-data/ sentence corpus version 1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK6cqCuZlagX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('rt-polarity.pos','r',encoding='latin-1')\n",
        "pos_rew_pres={}\n",
        "i=0\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1 and word.isalpha():\n",
        "      dic1[word]=1\n",
        "  dic1['label']=0\n",
        "  pos_rew_pres[i]=dic1\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVPGlsLyl0YR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('rt-polarity.neg','r',encoding='latin-1')\n",
        "neg_rew_pres={}\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1 and word.isalpha():\n",
        "      dic1[word]=1\n",
        "  dic1['label']=1\n",
        "  neg_rew_pres[i]=dic1\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J839D5RKl7hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f={**pos_rew_pres,**neg_rew_pres}\n",
        "df1=pd.DataFrame.from_dict(f,orient='index')\n",
        "df1.fillna(0,inplace=True)\n",
        "a=df1.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df1.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4U5daccl_IH",
        "colab_type": "code",
        "outputId": "ff765e67-65c2-442f-89ec-9b3f6850fa72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "##naive bayes\n",
        "y1=df1.label\n",
        "x1=df1.drop('label',axis=1)\n",
        "x1_train,x1_test,y1_train,y1_test=train_test_split(x1,y1,test_size=0.3,random_state=1234,stratify=df1.label)\n",
        "model2=GridSearchCV(MultinomialNB(),param_grid={},cv=10)\n",
        "model2.fit(x1_train,y1_train)\n",
        "y1_pred=model2.predict(x1_test)\n",
        "accuracy_score(y1_test,y1_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7661769302907159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC6ZQjLpC0-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##SVM\n",
        "model3=GridSearchCV(SVC(),param_grid={},cv=10)\n",
        "model3.fit(x1_train,y1_train)\n",
        "y1_pred1=model3.predict(x1_test)\n",
        "accuracy_score(y1_test,y1_pred1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EONsrLUKn8oi",
        "colab_type": "text"
      },
      "source": [
        "# creating the reviews without any proximity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqQQGJ1tmx2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g=list(df1.columns)\n",
        "g.remove('label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ1MXdXtAfYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makedataset(filename):\n",
        "  t=open(r'/content/drive/My Drive/txt_sentoken/pos' + '//' + filename,'r')\n",
        "  review=[]\n",
        "  sent=[]\n",
        "  for line in t:\n",
        "    for word in line.split():\n",
        "      if word=='.':\n",
        "        review.append(sent)\n",
        "        sent=[]\n",
        "      elif word.isalpha():\n",
        "        sent.append(word)\n",
        "  t.close()\n",
        "  return review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4g_L5AnzseE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLNM5KlPmCm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_cut_pos(list_sent):\n",
        "  probs=[]\n",
        "  line_sent=[]\n",
        "  for i in range(len(list_sent)):\n",
        "    dica={}\n",
        "    for a in g:\n",
        "      dica[a]=0\n",
        "    for word in list_sent[i]:\n",
        "      if word in dica and dica[word]==0:\n",
        "        dica[word]=dica[word]+1\n",
        "    df=pd.DataFrame.from_dict(dica,orient='index')\n",
        "    df=df.transpose()\n",
        "    pred=model2.predict_proba(df)\n",
        "    probs.append(pred[0][0])\n",
        "  top_idx = np.argsort(probs)[-5:]\n",
        "  for i in range(len(list_sent)):\n",
        "    if i in top_idx:\n",
        "      line_sent.append(list_sent[i])\n",
        "  return line_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2FIL7mBmHzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_cut_neg(list_sent):\n",
        "  dica={}\n",
        "  for a in g:\n",
        "    dica[a]=0\n",
        "  for word in list_sent:\n",
        "    if word in dica and dica[word]==0:\n",
        "      dica[word]=dica[word]+1\n",
        "  \n",
        "  df=pd.DataFrame.from_dict(dica,orient='index')\n",
        "  df=df.transpose()\n",
        "  if model2.predict(df)==[[0]]:\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R2VZqxtnya5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r=open('pos1.txt','w')\n",
        "count=0\n",
        "for file in files:\n",
        "  print(count)\n",
        "  line_sent=makedataset(file)\n",
        "  list_sent=min_cut_pos(line_sent)\n",
        "  for i in range(len(list_sent)):\n",
        "    for word in list_sent[i]:\n",
        "      r.write(word)\n",
        "      r.write('\\t')\n",
        "    r.write('\\n')\n",
        "\n",
        "  count=count+1\n",
        "r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR7VRkxZoyZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r=open('neg1.txt','w')\n",
        "for file in files1:\n",
        "  t=open(r'/content/drive/My Drive/txt_sentoken/neg' + '//' + file,'r')\n",
        "  line_sent=[]\n",
        "  for line in t:\n",
        "    for word in line.split():\n",
        "      if word=='.':\n",
        "        if min_cut(line_sent):\n",
        "          for w in line_sent:\n",
        "            r.write(w)\n",
        "            r.write('\\t')\n",
        "        line_sent=[]\n",
        "      elif word.isalpha():\n",
        "        line_sent.append(word)\n",
        "  r.write('\\n')\n",
        "  t.close()\n",
        "r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsRFiKhxoYC8",
        "colab_type": "text"
      },
      "source": [
        "# Building the polarity classifier on new review set formed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyEyWZgCpF9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('pos1.txt','r')\n",
        "pos_rew_pres1={}\n",
        "i=0\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1 and word.isalpha():\n",
        "      dic1[word]=1\n",
        "  dic1['label']=0\n",
        "  pos_rew_pres1[i]=dic1\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMByBpDapViH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('neg1.txt','r')\n",
        "neg_rew_pres1={}\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1 and word.isalpha():\n",
        "      dic1[word]=1\n",
        "  dic1['label']=1\n",
        "  neg_rew_pres1[i]=dic1\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMcRaXKJptYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f={**pos_rew_pres1,**neg_rew_pres1}\n",
        "df2=pd.DataFrame.from_dict(f,orient='index')\n",
        "df2.fillna(0,inplace=True)\n",
        "a=df2.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df2.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5rwOAPmps_N",
        "colab_type": "code",
        "outputId": "e4f4b813-93b7-4595-e424-155a2d7641d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y3=df2.label\n",
        "x3=df2.drop('label',axis=1)\n",
        "x3_train,x3_test,y3_train,y3_test=train_test_split(x3,y3,test_size=0.3,random_state=1234,stratify=df2.label)\n",
        "model3=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=10)\n",
        "model3.fit(x3_train,y3_train)\n",
        "y3_pred=model3.predict(x3_test)\n",
        "accuracy_score(y3_test,y3_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7683333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9fUCuVhDGb0",
        "colab_type": "code",
        "outputId": "d50e40e1-2495-4568-9a4e-bd9145c5ef2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model5=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model5.fit(x3_train,y3_train)\n",
        "y3_pred1=model5.predict(x3_test)\n",
        "accuracy_score(y3_test,y3_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7766666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGp5L8i3orYn",
        "colab_type": "text"
      },
      "source": [
        "# creating reviews with proximity function f(d)=1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu8Nck5-J1nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_cut_pr(line_sent):\n",
        "  dica={}\n",
        "  list1=[]\n",
        "  line1=[]\n",
        "  line2=[]\n",
        "  line3=[]\n",
        "  list_sent=[]\n",
        "  count=0\n",
        "  for word in line_sent:\n",
        "    if word=='.':\n",
        "      count=count+1\n",
        "    elif count==0:\n",
        "      line1.append(word)\n",
        "    elif count==1:\n",
        "      line2.append(word)\n",
        "    elif count==2:\n",
        "      line3.append(word)\n",
        "\n",
        "  for a in g:\n",
        "    dica[a]=0\n",
        "  for word in line_sent:\n",
        "    if word in dica and dica[word]==0:\n",
        "      dica[word]=1\n",
        "    elif word=='.':\n",
        "      df=pd.DataFrame.from_dict(dica,orient='index')\n",
        "      df=df.transpose()\n",
        "      list1.append((model2.predict_proba(df)).tolist())\n",
        "  \n",
        "  list1 = [item for sublist in list1 for item in sublist]\n",
        "  list1 = [item for sublist in list1 for item in sublist]\n",
        "  if len(list1)==6:\n",
        "    if list1[0]>0.5 or list1[2]>0.5 or list1[4]>0.5:\n",
        "      list_sent.append(line1)\n",
        "      list_sent.append(line3)\n",
        "      list_sent.append(line2)\n",
        "  list_sent = [item for sublist in list_sent for item in sublist]\n",
        "  \n",
        "  return list_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq_7uuGCJ12S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "###example to check the above function is working...\n",
        "t=open('cv000_29590.txt','r')\n",
        "line_sent=[]\n",
        "count=0\n",
        "for line in t:\n",
        "  for word in line.split():\n",
        "     if word=='.'and count==3:\n",
        "        print(min_cut_pr(line_sent))\n",
        "        line_sent=[]\n",
        "        count=0\n",
        "     elif word=='.':\n",
        "       count=count+1 \n",
        "       line_sent.append('.')   \n",
        "     elif word.isalpha():\n",
        "       line_sent.append(word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNKR8g6iz8I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r=open('pos2.txt','w')\n",
        "\n",
        "for file in files:\n",
        "  t=open(r'/content/drive/My Drive/txt_sentoken/pos' + '//' + file,'r')\n",
        "  line_sent=[]\n",
        "  for line in t:\n",
        "    for word in line.split():\n",
        "      if count==3:\n",
        "        list_sent=min_cut_pr(line_sent)\n",
        "        if list_sent!=[]:\n",
        "          for w in list_sent:\n",
        "            r.write(w)\n",
        "            r.write('\\t')\n",
        "        line_sent=[]\n",
        "        count=0\n",
        "      elif word=='.':\n",
        "        count=count+1 \n",
        "        line_sent.append('.')   \n",
        "      if word.isalpha():\n",
        "        line_sent.append(word)\n",
        "  r.write('\\n')\n",
        "  t.close()\n",
        "  \n",
        "r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJC3aQZ4gpR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_cut_prne(line_sent):\n",
        "  dica={}\n",
        "  list1=[]\n",
        "  line1=[]\n",
        "  line2=[]\n",
        "  line3=[]\n",
        "  list_sent=[]\n",
        "  count=0\n",
        "  for word in line_sent:\n",
        "    if word=='.':\n",
        "      count=count+1\n",
        "    elif count==0:\n",
        "      line1.append(word)\n",
        "    elif count==1:\n",
        "      line2.append(word)\n",
        "    elif count==2:\n",
        "      line3.append(word)\n",
        "\n",
        "  for a in g:\n",
        "    dica[a]=0\n",
        "  for word in line_sent:\n",
        "    if word in dica and dica[word]==0:\n",
        "      dica[word]=1\n",
        "    elif word=='.':\n",
        "      df=pd.DataFrame.from_dict(dica,orient='index')\n",
        "      df=df.transpose()\n",
        "      list1.append((model2.predict_proba(df)).tolist())\n",
        "  \n",
        "  list1 = [item for sublist in list1 for item in sublist]\n",
        "  list1 = [item for sublist in list1 for item in sublist]\n",
        "  if len(list1)==6:\n",
        "    if list1[1]>0.5 or list1[3]>0.5 or list1[5]>0.5:\n",
        "      list_sent.append(line1)\n",
        "      list_sent.append(line3)\n",
        "      list_sent.append(line2)\n",
        "  list_sent = [item for sublist in list_sent for item in sublist]\n",
        "  \n",
        "  return list_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ASOprjoNIdR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r=open('neg2.txt','w')\n",
        "for file in files1:\n",
        "  t=open(r'/content/drive/My Drive/txt_sentoken/neg' + '//' + file,'r')\n",
        "  line_sent=[]\n",
        "  for line in t:\n",
        "    for word in line.split():\n",
        "      if count==3:\n",
        "        list_sent=min_cut_prne(line_sent)\n",
        "        if list_sent!=[]:\n",
        "          for w in list_sent:\n",
        "            r.write(w)\n",
        "            r.write('\\t')\n",
        "        line_sent=[]\n",
        "        count=0\n",
        "      elif word=='.':\n",
        "        count=count+1 \n",
        "        line_sent.append('.')   \n",
        "      if word.isalpha():\n",
        "        line_sent.append(word)\n",
        "  r.write('\\n')\n",
        "  t.close()\n",
        "r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhPUNCsUpA-s",
        "colab_type": "text"
      },
      "source": [
        "# Building the polarity classifier on new review set formed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF4doA4qNJdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('pos2.txt','r')\n",
        "pos_rew_pres2={}\n",
        "i=0\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1 and word.isalpha():\n",
        "      dic1[word]=1\n",
        "  dic1['label']=0\n",
        "  pos_rew_pres2[i]=dic1\n",
        "  i=i+1\n",
        "t.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVbjbJlNVS1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('neg2.txt','r')\n",
        "neg_rew_pres2={}\n",
        "\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1 and word.isalpha():\n",
        "      dic1[word]=1\n",
        "  dic1['label']=1\n",
        "  pos_rew_pres2[i]=dic1\n",
        "  i=i+1\n",
        "t.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG1O2Dg4iArl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f={**pos_rew_pres2,**neg_rew_pres2}\n",
        "df3=pd.DataFrame.from_dict(f,orient='index')\n",
        "df3.fillna(0,inplace=True)\n",
        "a=df3.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df3.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CfUw-OKiLw4",
        "colab_type": "code",
        "outputId": "4d9f3c27-e632-4b51-c33c-7a8aa938e6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y4=df3.label\n",
        "x4=df3.drop('label',axis=1)\n",
        "x4_train,x4_test,y4_train,y4_test=train_test_split(x4,y4,test_size=0.3,random_state=1234,stratify=df3.label)\n",
        "model5=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=10)\n",
        "model5.fit(x4_train,y4_train)\n",
        "y4_pred=model5.predict(x4_test)\n",
        "accuracy_score(y4_test,y4_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW4J6RaTsD4w",
        "colab_type": "code",
        "outputId": "c043eaed-b0af-47a8-8447-76593a901572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model8=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model8.fit(x4_train,y4_train)\n",
        "y4_pred1=model5.predict(x4_test)\n",
        "accuracy_score(y4_test,y4_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-52rk6bCpKgF",
        "colab_type": "text"
      },
      "source": [
        "# Creating the review documents with proximity function f(d)=1/d^2 or    f(d)=e^(1-d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTQ0RyITjYyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_cut_pra(line_sent):\n",
        "  dica={}\n",
        "  list1=[]\n",
        "  line1=[]\n",
        "  line2=[]\n",
        "  line3=[]\n",
        "  list_sent=[]\n",
        "  count=0\n",
        "  for word in line_sent:\n",
        "    if word=='.':\n",
        "      count=count+1\n",
        "    elif count==0:\n",
        "      line1.append(word)\n",
        "    elif count==1:\n",
        "      line2.append(word)\n",
        "    elif count==2:\n",
        "      line3.append(word)\n",
        "\n",
        "  for a in g:\n",
        "    dica[a]=0\n",
        "  for word in line_sent:\n",
        "    if word in dica and dica[word]==0:\n",
        "      dica[word]=1\n",
        "    elif word=='.':\n",
        "      df=pd.DataFrame.from_dict(dica,orient='index')\n",
        "      df=df.transpose()\n",
        "      list1.append((model2.predict_proba(df)).tolist())\n",
        "  \n",
        "  list1 = [item for sublist in list1 for item in sublist]\n",
        "  list1 = [item for sublist in list1 for item in sublist]\n",
        "  if len(list1)==6:\n",
        "    if list1[0]>0.5:\n",
        "      list_sent.append(line1)\n",
        "      list_sent.append(line2)\n",
        "    elif list1[2]>0.5:\n",
        "      list_sent.append(line1)\n",
        "      list_sent.append(line2)\n",
        "      list_sent.append(line3)\n",
        "    elif list1[4]>0.5:\n",
        "      list_sent.append(line2)\n",
        "      list_sent.append(line3)\n",
        "      \n",
        "  list_sent = [item for sublist in list_sent for item in sublist]\n",
        "  \n",
        "  return list_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywqdf0JIjYlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r=open('pos3.txt','w')\n",
        "for file in files:\n",
        "  t=open(r'/content/drive/My Drive/txt_sentoken/pos' + '//' + file,'r')\n",
        "  line_sent=[]\n",
        "  for line in t:\n",
        "    for word in line.split():\n",
        "      if count==3:\n",
        "        list_sent=min_cut_pra(line_sent)\n",
        "        if list_sent!=[]:\n",
        "          for w in list_sent:\n",
        "            r.write(w)\n",
        "            r.write('\\t')\n",
        "        line_sent=[]\n",
        "        count=0\n",
        "      elif word=='.':\n",
        "        count=count+1 \n",
        "        line_sent.append('.')   \n",
        "      if word.isalpha():\n",
        "        line_sent.append(word)\n",
        "  r.write('\\n')\n",
        "  t.close()\n",
        "r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfIL0O8ojYYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK6_Txz7iihn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def min_cut_prane(line_sent):\n",
        "  dica={}\n",
        "  list1=[]\n",
        "  line1=[]\n",
        "  line2=[]\n",
        "  line3=[]\n",
        "  list_sent=[]\n",
        "  count=0\n",
        "  for word in line_sent:\n",
        "    if word=='.':\n",
        "      count=count+1\n",
        "    elif count==0:\n",
        "      line1.append(word)\n",
        "    elif count==1:\n",
        "      line2.append(word)\n",
        "    elif count==2:\n",
        "      line3.append(word)\n",
        "\n",
        "  for a in g:\n",
        "    dica[a]=0\n",
        "  for word in line_sent:\n",
        "    if word in dica and dica[word]==0:\n",
        "      dica[word]=1\n",
        "    elif word=='.':\n",
        "      df=pd.DataFrame.from_dict(dica,orient='index')\n",
        "      df=df.transpose()\n",
        "      list1.append((model2.predict_proba(df)).tolist())\n",
        "  \n",
        "  list1 = [item for sublist in list1 for item in sublist]\n",
        "  list1 = [item for sublist in list1 for item in sublist]\n",
        "  if len(list1)==6:\n",
        "    if list1[0]>0.5:\n",
        "      list_sent.append(line1)\n",
        "      list_sent.append(line2)\n",
        "    elif list1[2]>0.5:\n",
        "      list_sent.append(line1)\n",
        "      list_sent.append(line2)\n",
        "      list_sent.append(line3)\n",
        "    elif list1[4]>0.5:\n",
        "      list_sent.append(line2)\n",
        "      list_sent.append(line3)\n",
        "      \n",
        "  list_sent = [item for sublist in list_sent for item in sublist]\n",
        "  \n",
        "  return list_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEdEx2S3kzeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r=open('neg3.txt','w')\n",
        "for file in files1:\n",
        "  t=open(r'/content/drive/My Drive/txt_sentoken/neg' + '//' + file,'r')\n",
        "  line_sent=[]\n",
        "  for line in t:\n",
        "    for word in line.split():\n",
        "      if count==3:\n",
        "        list_sent=min_cut_prane(line_sent)\n",
        "        if list_sent!=[]:\n",
        "          for w in list_sent:\n",
        "            r.write(w)\n",
        "            r.write('\\t')\n",
        "        line_sent=[]\n",
        "        count=0\n",
        "      elif word=='.':\n",
        "        count=count+1 \n",
        "        line_sent.append('.')   \n",
        "      if word.isalpha():\n",
        "        line_sent.append(word)\n",
        "  r.write('\\n')\n",
        "  t.close()\n",
        "r.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOMswg62ppyR",
        "colab_type": "text"
      },
      "source": [
        "# Building the polarity classifiers with new review documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LqWwjm0lGiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('pos3.txt','r')\n",
        "pos_rew_pres3={}\n",
        "i=0\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1 and word.isalpha():\n",
        "      dic1[word]=1\n",
        "  dic1['label']=0\n",
        "  pos_rew_pres3[i]=dic1\n",
        "  i=i+1\n",
        "t.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N25QiLgrlPAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('neg3.txt','r')\n",
        "neg_rew_pres3={}\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1 and word.isalpha():\n",
        "      dic1[word]=1\n",
        "  dic1['label']=1\n",
        "  neg_rew_pres3[i]=dic1\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF3aW2zzlab8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f={**pos_rew_pres3,**neg_rew_pres3}\n",
        "df4=pd.DataFrame.from_dict(f,orient='index')\n",
        "df4.fillna(0,inplace=True)\n",
        "a=df4.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df4.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3JgztGYlwM8",
        "colab_type": "code",
        "outputId": "7eeaa2d0-746c-469f-d564-d3b74339e50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y5=df4.label\n",
        "x5=df4.drop('label',axis=1)\n",
        "x5_train,x5_test,y5_train,y5_test=train_test_split(x5,y5,test_size=0.3,random_state=1234,stratify=df4.label)\n",
        "model6=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=10)\n",
        "model6.fit(x5_train,y5_train)\n",
        "y5_pred=model6.predict(x5_test)\n",
        "accuracy_score(y5_test,y5_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKNDF082no_S",
        "colab_type": "code",
        "outputId": "44cffcb6-e071-4b06-b2e3-e6e5b2c85471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model5=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model5.fit(x5_train,y5_train)\n",
        "y5_pred1=model5.predict(x5_test)\n",
        "accuracy_score(y5_test,y5_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.76"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ld_P2WbtYcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}