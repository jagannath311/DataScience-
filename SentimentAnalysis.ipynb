{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1dE7y4JbbMGUCEDu9NlN-E4JY8ln4eLAZ",
      "authorship_tag": "ABX9TyPQmBwWScbKvosXWTBM5Bk5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagannath311/DataScience-/blob/master/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7xaxgLRrFKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##the required libraries are following\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import operator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iALM5pCdF6du",
        "colab_type": "text"
      },
      "source": [
        "# Converting the thousand files of positive reviews and negative reviews into single text files where in which one line contains the words of one document respectively for easy access in later part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0yE2BQZrLss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import collections\n",
        "files=os.listdir(r'/content/drive/My Drive/txt_sentoken/pos')\n",
        "files1=os.listdir(r'/content/drive/My Drive/txt_sentoken/neg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITFNSBMnrOo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z=open('pos.txt','w')\n",
        "for file in files:\n",
        "  t=open(r'/content/drive/My Drive/txt_sentoken/pos' + '//' + file,'r')\n",
        "  for line in t:\n",
        "    for word in line.split():\n",
        "      if word.isalpha():\n",
        "        z.write(word)\n",
        "        z.write('\\t')\n",
        "  t.close()\n",
        "  z.write('\\n')\n",
        "z.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdK7KicxrW-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z=open('neg.txt','w')\n",
        "for file in files1:\n",
        "  t=open(r'/content/drive/My Drive/txt_sentoken/neg' + '//' + file,'r')\n",
        "  for line in t:\n",
        "    for word in line.split():\n",
        "      if word.isalpha():\n",
        "        z.write(word)\n",
        "        z.write('\\t')\n",
        "  t.close()\n",
        "  z.write('\\n') \n",
        "z.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f9axmpPGe6a",
        "colab_type": "text"
      },
      "source": [
        "# creation of unigrams which contains features as frequency and presence respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUDY_Erkr4eO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('pos.txt','r')\n",
        "pos_rew_freq={}\n",
        "pos_rew_pres={}\n",
        "i=0\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  dic2={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1:\n",
        "      dic1[word]=1\n",
        "      dic2[word]=1\n",
        "    else:\n",
        "      dic1[word]=dic1[word]+1\n",
        "  dic1['label']=0\n",
        "  dic2['label']=0\n",
        "  pos_rew_freq[i]=dic1\n",
        "  pos_rew_pres[i]=dic2\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha0raPjEuBmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t=open('neg.txt','r')\n",
        "neg_rew_freq={}\n",
        "neg_rew_pres={}\n",
        "\n",
        "for line in t:\n",
        "  dic1={}\n",
        "  dic2={}\n",
        "  for word in line.split():\n",
        "    if word not in dic1:\n",
        "      dic1[word]=1\n",
        "      dic2[word]=1\n",
        "    else:\n",
        "      dic1[word]=dic1[word]+1\n",
        "  dic1['label']=1\n",
        "  dic2['label']=1\n",
        "  neg_rew_freq[i]=dic1\n",
        "  neg_rew_pres[i]=dic2\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0GN-OCwHEQg",
        "colab_type": "text"
      },
      "source": [
        "# Buiding the model on the unigrams which contain frequency as features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkogPYmlvOu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f={**pos_rew_freq,**neg_rew_freq}\n",
        "df=pd.DataFrame.from_dict(f,orient='index')\n",
        "df.fillna(0,inplace=True)\n",
        "g=list(df.columns)\n",
        "a=df.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-_A317svOU-",
        "colab_type": "code",
        "outputId": "f22a45c8-8a83-4afd-e3b5-ee78311b525f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y=df.label\n",
        "x=df.drop('label',axis=1)\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=1234,stratify=df.label)\n",
        "model=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=3)\n",
        "model.fit(x_train,y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.81"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnBZEYZ6wBx9",
        "colab_type": "code",
        "outputId": "70ac3833-3c3a-4a5a-f614-a2ac30a707d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model1=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model1.fit(x_train,y_train)\n",
        "y_pred1=model1.predict(x_test)\n",
        "accuracy_score(y_test,y_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7816666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsqyuZ6wHxZc",
        "colab_type": "text"
      },
      "source": [
        "# Bulding the model on the unigrams which contain presence as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DAiYPd7xc6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f={**pos_rew_pres,**neg_rew_pres}\n",
        "df1=pd.DataFrame.from_dict(f,orient='index')\n",
        "df1.fillna(0,inplace=True)\n",
        "a=df1.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df1.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc1XYX7nxqx_",
        "colab_type": "code",
        "outputId": "b6c3eab9-64bd-45af-e1df-e6267e963ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y1=df1.label\n",
        "x1=df1.drop('label',axis=1)\n",
        "x1_train,x1_test,y1_train,y1_test=train_test_split(x1,y1,test_size=0.3,random_state=1234,stratify=df1.label)\n",
        "model2=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=3)\n",
        "model2.fit(x1_train,y1_train)\n",
        "y1_pred=model2.predict(x1_test)\n",
        "accuracy_score(y1_test,y1_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8283333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsdckm7LyEBr",
        "colab_type": "code",
        "outputId": "f0b110f1-9c4b-4d81-f40b-5a1eb63669b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model3=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model3.fit(x1_train,y1_train)\n",
        "y1_pred1=model3.predict(x1_test)\n",
        "accuracy_score(y1_test,y1_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8616666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SQ0nOWAyXRB",
        "colab_type": "code",
        "outputId": "792fdc79-490c-4dbb-fcdd-1f36819e7a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model4=GridSearchCV(make_pipeline(StandardScaler(),LogisticRegression(penalty='l2',random_state=123)),param_grid={},cv=3)\n",
        "model4.fit(x1_train,y1_train)\n",
        "y1_pred2=model4.predict(x1_test)\n",
        "accuracy_score(y_test,y1_pred2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8416666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNXFp4ksuYZC",
        "colab_type": "code",
        "outputId": "23dbf8a5-2fd3-42ac-b530-778b5eef2d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FftKNYrzIK2V",
        "colab_type": "text"
      },
      "source": [
        "# Building the model as the bigrams+unigrams as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LB7owXW67Sm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_rew_biuni={}\n",
        "i=0\n",
        "t=open('pos.txt','r')\n",
        "for line in t:\n",
        "  dic={}\n",
        "  list1=[]\n",
        "  for word in line.split():\n",
        "    if word not in dic:\n",
        "      dic[word]=1\n",
        "      list1.append(word)\n",
        "  for a in list1:\n",
        "    dic[a]=1\n",
        "  dic['label']=0\n",
        "  pos_rew_biuni[i]=dic\n",
        "  i=i+1\n",
        "t.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AgZeKsD8oT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg_rew_biuni={}\n",
        "t=open('neg.txt','r')\n",
        "for line in t:\n",
        "  dic={}\n",
        "  list1=[]\n",
        "  for word in line.split():\n",
        "    if word not in dic:\n",
        "      dic[word]=1\n",
        "      list1.append(word)\n",
        "  for a in list1:\n",
        "    dic[a]=1\n",
        "  dic['label']=1\n",
        "  neg_rew_biuni[i]=dic\n",
        "  i=i+1\n",
        "t.close() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9JZk1Bz8wM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1={**pos_rew_biuni,**neg_rew_biuni}\n",
        "\n",
        "df3=pd.DataFrame.from_dict(f1,orient='index')\n",
        "df3.fillna(0,inplace=True)\n",
        "a=df3.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df3.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E3RpDKp817A",
        "colab_type": "code",
        "outputId": "f80733a7-f878-4816-ebf9-0398769c8276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y3=df3.label\n",
        "x3=df3.drop('label',axis=1)\n",
        "x3_train,x3_test,y3_train,y3_test=train_test_split(x3,y3,test_size=0.3,random_state=1234,stratify=df3.label)\n",
        "model8=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=3)\n",
        "model8.fit(x3_train,y3_train)\n",
        "y3_pred=model8.predict(x3_test)\n",
        "accuracy_score(y3_test,y3_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rzh1R319HNB",
        "colab_type": "code",
        "outputId": "29e857fd-8c56-49e0-d1bc-5338f2a093cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model9=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model9.fit(x3_train,y3_train)\n",
        "y3_pred1=model9.predict(x3_test)\n",
        "accuracy_score(y3_test,y3_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8616666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed7pU6i49R-C",
        "colab_type": "code",
        "outputId": "2e5346db-36a3-4292-b994-e0fe83a48eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model10=GridSearchCV(make_pipeline(StandardScaler(),LogisticRegression(penalty='l2',random_state=123)),param_grid={},cv=3)\n",
        "model10.fit(x3_train,y3_train)\n",
        "y3_pred2=model10.predict(x3_test)\n",
        "accuracy_score(y3_test,y3_pred2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWMYoG6sIYA5",
        "colab_type": "text"
      },
      "source": [
        "# Building the model with unigram and position as the feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N55ubAP4-CPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_rew_uniposition={}\n",
        "i=0\n",
        "t=open('pos.txt','r')\n",
        "for line in t:\n",
        "  dic={}\n",
        "  j='0'\n",
        "  k=0\n",
        "  for word in line.split():\n",
        "    word=word+'_'+j\n",
        "    if word not in dic:\n",
        "      dic[word]=1\n",
        "    if k==len(line.split())/4:\n",
        "      j='1'\n",
        "    if k==3*(len(line.split()))/4:\n",
        "      j='2'\n",
        "    k=k+1 \n",
        "  dic['label']=0\n",
        "  pos_rew_uniposition[i]=dic\n",
        "  i=i+1\n",
        "t.close()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjfqZFqdB89l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg_rew_uniposition={}\n",
        "t=open('neg.txt','r')\n",
        "for line in t:\n",
        "  dic={}\n",
        "  j='0'\n",
        "  for word in line.split():\n",
        "    word=word+'_'+j\n",
        "    if word not in dic:\n",
        "      dic[word]=1\n",
        "    if k==len(line.split())/4:\n",
        "      j='1'\n",
        "    if k==3*(len(line.split()))/4:\n",
        "      j='2'\n",
        "    k=k+1 \n",
        "  dic['label']=1\n",
        "  neg_rew_uniposition[i]=dic\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mqRgdGXDu1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1={**pos_rew_uniposition,**neg_rew_uniposition}\n",
        "\n",
        "df4=pd.DataFrame.from_dict(f1,orient='index')\n",
        "df4.fillna(0,inplace=True)\n",
        "a=df4.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df4.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNuE8xgVD7MG",
        "colab_type": "code",
        "outputId": "16a45126-7e3a-4942-a293-a4ca17a53b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y4=df4.label\n",
        "x4=df4.drop('label',axis=1)\n",
        "x4_train,x4_test,y4_train,y4_test=train_test_split(x4,y4,test_size=0.3,random_state=1234,stratify=df4.label)\n",
        "model11=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=3)\n",
        "model11.fit(x4_train,y4_train)\n",
        "y4_pred=model11.predict(x4_test)\n",
        "accuracy_score(y4_test,y4_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6533333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBB6s9unK3pM",
        "colab_type": "code",
        "outputId": "0289a9a6-e31a-4272-ab37-f1d86ba22dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model12=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model12.fit(x4_train,y4_train)\n",
        "y4_pred1=model12.predict(x4_test)\n",
        "accuracy_score(y4_test,y4_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8766666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC8njiQrLLF9",
        "colab_type": "code",
        "outputId": "e8284558-e89a-49be-fa34-29811a18147a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model13=GridSearchCV(make_pipeline(StandardScaler(),LogisticRegression(penalty='l2',random_state=123)),param_grid={},cv=3)\n",
        "model13.fit(x4_train,y4_train)\n",
        "y4_pred2=model13.predict(x4_test)\n",
        "accuracy_score(y4_test,y4_pred2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8616666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7aHu-qFIjla",
        "colab_type": "text"
      },
      "source": [
        "# Building the model with top 2633 unigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3qAWMxKMpjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f={**pos_rew_pres,**neg_rew_pres}\n",
        "df5=pd.DataFrame.from_dict(f,orient='index')\n",
        "df5.fillna(0,inplace=True)\n",
        "a=df5.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzHMlWleSwJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted_d = dict(sorted(a.items(), key=operator.itemgetter(1),reverse=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWE2SphdPAnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k=0\n",
        "list1=[]\n",
        "list2=[]\n",
        "for key,value in sorted_d.items():\n",
        "  if k<2633:\n",
        "    list1.append(key)\n",
        "  else:\n",
        "    list2.append(key)\n",
        "  k=k+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcOx-IJRPLBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df5.drop(list2,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "160RZo7IQakw",
        "colab_type": "code",
        "outputId": "4cee8d6c-0936-4a65-97db-101511b6c889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y5=df5.label\n",
        "x5=df5.drop('label',axis=1)\n",
        "x5_train,x5_test,y5_train,y5_test=train_test_split(x5,y5,test_size=0.3,random_state=1234,stratify=df5.label)\n",
        "model14=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=3)\n",
        "model14.fit(x5_train,y5_train)\n",
        "y5_pred=model14.predict(x5_test)\n",
        "accuracy_score(y5_test,y5_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8433333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0Net71MQvss",
        "colab_type": "code",
        "outputId": "a1cd6ebc-f281-4f19-bff2-c99e5c04faa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model15=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model15.fit(x5_train,y5_train)\n",
        "y5_pred1=model15.predict(x5_test)\n",
        "accuracy_score(y5_test,y5_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8633333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljrVjOfaxF_n",
        "colab_type": "code",
        "outputId": "0bfb2bbf-302b-4d58-f7e5-86e18a8455c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model16=GridSearchCV(make_pipeline(StandardScaler(),LogisticRegression(penalty='l2',random_state=123)),param_grid={},cv=3)\n",
        "model16.fit(x5_train,y5_train)\n",
        "y5_pred2=model16.predict(x5_test)\n",
        "accuracy_score(y5_test,y5_pred2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQFFtJ5UIsm2",
        "colab_type": "text"
      },
      "source": [
        "# Building the model with unigrams +POS tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2ubYY0T21Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_rew_unipos={}\n",
        "t=open('pos.txt','r')\n",
        "i=0\n",
        "for line in t:\n",
        "  dic={}\n",
        "  words=nltk.pos_tag(line.split())\n",
        "  for word in words:\n",
        "    if word not in dic:\n",
        "      dic[word]=1\n",
        "  dic['label']=0\n",
        "  pos_rew_unipos[i]=dic\n",
        "  i=i+1\n",
        "t.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_HIVWwv4VO6",
        "colab_type": "code",
        "outputId": "008a1a3e-bd0b-4903-e115-ff538cdc902d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYYoC8vW4c73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg_rew_unipos={}\n",
        "t=open('neg.txt','r')\n",
        "for line in t:\n",
        "  dic={}\n",
        "  words=nltk.pos_tag(line.split())\n",
        "  for word in words:\n",
        "    if word not in dic:\n",
        "      dic[word]=1\n",
        "  dic['label']=1\n",
        "  neg_rew_unipos[i]=dic\n",
        "  i=i+1\n",
        "t.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2TYNGFX4rwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1={**pos_rew_unipos,**neg_rew_unipos}\n",
        "\n",
        "df6=pd.DataFrame.from_dict(f1,orient='index')\n",
        "df6.fillna(0,inplace=True)\n",
        "a=df6.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df6.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCPWQVEbZQ7F",
        "colab_type": "code",
        "outputId": "07f36339-5542-4320-da47-10fc63f97422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "df6.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(i, NN)</th>\n",
              "      <th>(am, VBP)</th>\n",
              "      <th>(starting, VBG)</th>\n",
              "      <th>(to, TO)</th>\n",
              "      <th>(write, VB)</th>\n",
              "      <th>(this, DT)</th>\n",
              "      <th>(review, NN)</th>\n",
              "      <th>(before, IN)</th>\n",
              "      <th>(going, VBG)</th>\n",
              "      <th>(see, VB)</th>\n",
              "      <th>(and, CC)</th>\n",
              "      <th>(oscar, NN)</th>\n",
              "      <th>(nominee, NN)</th>\n",
              "      <th>(for, IN)</th>\n",
              "      <th>(best, JJS)</th>\n",
              "      <th>(foreign, JJ)</th>\n",
              "      <th>(film, NN)</th>\n",
              "      <th>(directed, VBN)</th>\n",
              "      <th>(by, IN)</th>\n",
              "      <th>(who, WP)</th>\n",
              "      <th>(also, RB)</th>\n",
              "      <th>(directed, VBD)</th>\n",
              "      <th>(the, DT)</th>\n",
              "      <th>(sometimes, NNS)</th>\n",
              "      <th>(in, IN)</th>\n",
              "      <th>(some, DT)</th>\n",
              "      <th>(circles, NNS)</th>\n",
              "      <th>(critically, RB)</th>\n",
              "      <th>(acclaimed, VBD)</th>\n",
              "      <th>(memories, NNS)</th>\n",
              "      <th>(of, IN)</th>\n",
              "      <th>(saw, VBD)</th>\n",
              "      <th>(as, IN)</th>\n",
              "      <th>(part, NN)</th>\n",
              "      <th>(a, DT)</th>\n",
              "      <th>(cuban, JJ)</th>\n",
              "      <th>(cinema, NN)</th>\n",
              "      <th>(class, NN)</th>\n",
              "      <th>(back, RB)</th>\n",
              "      <th>(late, JJ)</th>\n",
              "      <th>...</th>\n",
              "      <th>(de, JJ)</th>\n",
              "      <th>(lennox, NN)</th>\n",
              "      <th>(alicia, VBP)</th>\n",
              "      <th>(libido, NN)</th>\n",
              "      <th>(hoodlum, NN)</th>\n",
              "      <th>(roaming, VBG)</th>\n",
              "      <th>(beware, NN)</th>\n",
              "      <th>(bit, JJ)</th>\n",
              "      <th>(lackluster, NN)</th>\n",
              "      <th>(mir, NN)</th>\n",
              "      <th>(mona, NN)</th>\n",
              "      <th>(heckerling, NN)</th>\n",
              "      <th>(dependable, JJ)</th>\n",
              "      <th>(banged, VBN)</th>\n",
              "      <th>(leagues, NNS)</th>\n",
              "      <th>(sphere, NN)</th>\n",
              "      <th>(levinson, NN)</th>\n",
              "      <th>(baywatch, NN)</th>\n",
              "      <th>(possess, VBP)</th>\n",
              "      <th>(negativity, NN)</th>\n",
              "      <th>(disappointing, NN)</th>\n",
              "      <th>(seconds, VBZ)</th>\n",
              "      <th>(tanker, NN)</th>\n",
              "      <th>(police, VBP)</th>\n",
              "      <th>(cinemax, NN)</th>\n",
              "      <th>(pacula, NN)</th>\n",
              "      <th>(cook, VB)</th>\n",
              "      <th>(wb, NN)</th>\n",
              "      <th>(plummets, NNS)</th>\n",
              "      <th>(voted, VBN)</th>\n",
              "      <th>(unexciting, JJ)</th>\n",
              "      <th>(wanda, NN)</th>\n",
              "      <th>(devious, JJ)</th>\n",
              "      <th>(rifle, NN)</th>\n",
              "      <th>(spawn, VBN)</th>\n",
              "      <th>(bounce, NN)</th>\n",
              "      <th>(unacceptable, JJ)</th>\n",
              "      <th>(tattoos, NN)</th>\n",
              "      <th>(overwrought, JJ)</th>\n",
              "      <th>(stereotyped, VBN)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  17796 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   (i, NN)  (am, VBP)  ...  (overwrought, JJ)  (stereotyped, VBN)\n",
              "0      1.0        1.0  ...                0.0                 0.0\n",
              "1      1.0        1.0  ...                0.0                 0.0\n",
              "2      1.0        0.0  ...                0.0                 0.0\n",
              "4      1.0        0.0  ...                0.0                 0.0\n",
              "7      1.0        0.0  ...                0.0                 0.0\n",
              "\n",
              "[5 rows x 17796 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4BPMh-l45VY",
        "colab_type": "code",
        "outputId": "7a245526-c88e-47ba-befc-1263e96fe8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y6=df6.label\n",
        "x6=df6.drop('label',axis=1)\n",
        "x6_train,x6_test,y6_train,y6_test=train_test_split(x6,y6,test_size=0.3,random_state=1234,stratify=df6.label)\n",
        "model17=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=3)\n",
        "model17.fit(x6_train,y6_train)\n",
        "y6_pred=model17.predict(x6_test)\n",
        "accuracy_score(y6_test,y6_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8066666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OiL7yvF5JCL",
        "colab_type": "code",
        "outputId": "1aacd1f1-e1bc-418a-cf6b-5db1c23a8f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model18=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model18.fit(x6_train,y6_train)\n",
        "y6_pred1=model18.predict(x6_test)\n",
        "accuracy_score(y6_test,y6_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8433333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txjywxIp5aUA",
        "colab_type": "code",
        "outputId": "f33d5ee4-690f-466d-a86a-25409854be4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model19=GridSearchCV(make_pipeline(StandardScaler(),LogisticRegression(penalty='l2',random_state=123)),param_grid={},cv=3)\n",
        "model19.fit(x6_train,y6_train)\n",
        "y6_pred2=model19.predict(x6_test)\n",
        "accuracy_score(y6_test,y6_pred2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8183333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MkWO0NjI1Z3",
        "colab_type": "text"
      },
      "source": [
        "# Building the model the presence of adjectives as features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2EafakU5lji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_rew_uniadj={}\n",
        "t=open('pos.txt','r')\n",
        "i=0\n",
        "lista=['JJ','JJR','JJS']\n",
        "for line in t:\n",
        "  dic={}\n",
        "  words=nltk.pos_tag(line.split())\n",
        "  for word in words:\n",
        "    if word not in dic and word[1] in lista:\n",
        "      dic[word[0]]=1\n",
        "  dic['label']=0\n",
        "  pos_rew_uniadj[i]=dic\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43G5WN0Z6qFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg_rew_uniadj={}\n",
        "t=open('neg.txt','r')\n",
        "\n",
        "for line in t:\n",
        "  dic={}\n",
        "  words=nltk.pos_tag(line.split())\n",
        "  for word in words:\n",
        "    if word not in dic and word[1] in lista:\n",
        "      dic[word[0]]=1\n",
        "  dic['label']=1\n",
        "  neg_rew_uniadj[i]=dic\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9txO2RDW65t8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1={**pos_rew_uniadj,**neg_rew_uniadj}\n",
        "\n",
        "df7=pd.DataFrame.from_dict(f1,orient='index')\n",
        "df7.fillna(0,inplace=True)\n",
        "a=df7.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df7.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXhzo5rv7DB4",
        "colab_type": "code",
        "outputId": "526c53b5-31b0-4421-c3d4-c68b908187e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y7=df7.label\n",
        "x7=df7.drop('label',axis=1)\n",
        "x7_train,x7_test,y7_train,y7_test=train_test_split(x7,y7,test_size=0.3,random_state=1234,stratify=df7.label)\n",
        "model20=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=3)\n",
        "model20.fit(x7_train,y7_train)\n",
        "y7_pred=model20.predict(x7_test)\n",
        "accuracy_score(y7_test,y7_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7966666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZb62OEH7SIc",
        "colab_type": "code",
        "outputId": "65c7fbde-8920-44e2-af86-781de3e25d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model21=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model21.fit(x7_train,y7_train)\n",
        "y7_pred1=model21.predict(x7_test)\n",
        "accuracy_score(y7_test,y7_pred1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.79"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7AKFm3x7eQx",
        "colab_type": "code",
        "outputId": "12a702c6-66b8-49c5-937a-c1f0ae90d444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model22=GridSearchCV(make_pipeline(StandardScaler(),LogisticRegression(penalty='l2',random_state=123)),param_grid={},cv=3)\n",
        "model22.fit(x7_train,y7_train)\n",
        "y7_pred2=model22.predict(x7_test)\n",
        "accuracy_score(y7_test,y7_pred2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7033333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVEjl0s-ICuu",
        "colab_type": "text"
      },
      "source": [
        "# Building the model with features as bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhSJWJ0Zynle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_rew_bigrams={}\n",
        "i=0\n",
        "t=open('pos.txt','rb')\n",
        "for line in t:\n",
        "  dic={}\n",
        "  list1=[]\n",
        "  for word in line.split():\n",
        "    if word not in list1:\n",
        "      list1.append(word)\n",
        "  bigrams=nltk.bigrams(list1)\n",
        "  for a in bigrams:\n",
        "    word=a[0]+a[1]\n",
        "    word=str(word)\n",
        "    dic[word]=1\n",
        "  dic['label']=0\n",
        "  pos_rew_bigrams[i]=dic\n",
        "  i=i+1\n",
        "t.close()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL-BDrGEzhol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg_rew_bigrams={}\n",
        "t=open('neg.txt','r')\n",
        "for line in t:\n",
        "  dic={}\n",
        "  list1=[]\n",
        "  for word in line.split():\n",
        "    if word not in list1:\n",
        "      list1.append(word)\n",
        "  list1=nltk.bigrams(list1)\n",
        "  for a in list1:\n",
        "    word=a[0]+a[1]\n",
        "    word=str(word)\n",
        "    dic[word]=1\n",
        "  dic['label']=1\n",
        "  neg_rew_bigrams[i]=dic\n",
        "  i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ITmWbkKz0hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f1={**pos_rew_bigrams,**neg_rew_bigrams}\n",
        "\n",
        "df2=pd.DataFrame.from_dict(f1,orient='index')\n",
        "df2.fillna(0,inplace=True)\n",
        "a=df2.sum(axis=0)\n",
        "a=a.to_dict()\n",
        "list1=[]\n",
        "for key,value in list(a.items()):\n",
        "  if value<4:\n",
        "    list1.append(key)\n",
        "df2.drop(list1,axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slEV-zDS3XQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y2=df2.label\n",
        "x2=df2.drop('label',axis=1)\n",
        "x2_train,x2_test,y2_train,y2_test=train_test_split(x2,y2,test_size=0.3,random_state=1234,stratify=df2.label)\n",
        "model5=GridSearchCV(make_pipeline(MinMaxScaler(),MultinomialNB()),param_grid={'multinomialnb__alpha':[1]},cv=3)\n",
        "model5.fit(x2_train,y2_train)\n",
        "y2_pred=model5.predict(x2_test)m\n",
        "accuracy_score(y2_test,y2_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS0N2LJB6Jnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model6=GridSearchCV(SVC(),param_grid={},cv=3)\n",
        "model6.fit(x2_train,y2_train)\n",
        "y2_pred1=model6.predict(x2_test)\n",
        "accuracy_score(y2_test,y2_pred1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e17DxNNq6p5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model7=GridSearchCV(make_pipeline(StandardScaler(),LogisticRegression(penalty='l2',random_state=123)),param_grid={},cv=3)\n",
        "model7.fit(x2_train,y2_train)\n",
        "y2_pred2=model7.predict(x2_test)\n",
        "accuracy_score(y2_test,y2_pred2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ee6RwvVi4Wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}