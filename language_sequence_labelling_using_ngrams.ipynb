{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language sequence labelling using ngrams.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPhd9urM12dflHauuo9FOS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jagannath311/DataScience-/blob/master/language_sequence_labelling_using_ngrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjXpfDGRyhG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "import linecache\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxyLXq_HyqPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###it is a function to make a word into its constituent ngrams\n",
        "def ngrams(n,word):\n",
        "    list1=[]\n",
        "    for i in range(len(word)):\n",
        "            list1.append(word[i])\n",
        "    for i in range(len(word)-1):\n",
        "            list1.append(word[i]+word[i+1])\n",
        "    for i in range(len(word)-2):\n",
        "            list1.append(word[i]+word[i+1]+word[i+2])\n",
        "    for i in range(len(word)-3):\n",
        "            list1.append(word[i]+word[i+1]+word[i+2]+word[i+3])\n",
        "    for i in range(len(word)-4):\n",
        "            list1.append(word[i]+word[i+1]+word[i+2]+word[i+3]+word[i+4])\n",
        "    return list1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADhmturYzKHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##constructing a ngram file for each language\n",
        "def ngramlan(x,y):\n",
        "  t=open(x,'r')\n",
        "  w=open(y,'w')\n",
        "\n",
        "  for line in t:\n",
        "    for word in line.split():\n",
        "        l=[]\n",
        "        l=ngrams(5,word)\n",
        "        \n",
        "        for i in l:\n",
        "            w.write(i)\n",
        "            w.write('\\t')\n",
        "        w.write('\\n')\n",
        "  t.close()\n",
        "  w.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdkGVxe1zwI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##already english ngrams are given making a dictionary out of it\n",
        "t=open('enngrams.txt','r')\n",
        "dic={}\n",
        "enngrams={}\n",
        "l=0\n",
        "i=0\n",
        "for line in t:\n",
        "    for word in line.split():\n",
        "        if l==0:\n",
        "            if word not in dic:\n",
        "                dic[word]=1\n",
        "            else:\n",
        "                dic[word]=dic[word]+1\n",
        "    \n",
        "        else:\n",
        "            if len(word)==1:\n",
        "                dic['label']='en'\n",
        "                enngrams[i]=dic\n",
        "                \n",
        "                i=i+1\n",
        "                dic={}\n",
        "                dic[word]=1\n",
        "                l=0\n",
        "            else:\n",
        "                l=0\n",
        "                if word not in dic:\n",
        "                    dic[word]=1\n",
        "                else:\n",
        "                    dic[word]=dic[word]+1\n",
        "    l=l+1\n",
        "    \n",
        "    \n",
        "t.close()\n",
        "e=i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybe_-dsm18F9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##training the each language classifier\n",
        "def trainingBclassifier(a,b,c,fitted_models,labels):\n",
        "  t=open(c,'r')\n",
        "  i=e\n",
        "  lanngrams={}\n",
        "  f={}\n",
        "  g=[]\n",
        "  for line in t:\n",
        "    dic={}\n",
        "    for word in line.split():\n",
        "      if word not in dic and word.isalpha():\n",
        "          dic[word]=1\n",
        "      elif word.isalpha():\n",
        "          dic[word]=dic[word]+1\n",
        "    dic['label']=labels\n",
        "    \n",
        "    lanngrams[i]=dic\n",
        "    i=i+1\n",
        "  t.close()\n",
        "  \n",
        "  f={**b,**lanngrams}\n",
        "  df=pd.DataFrame.from_dict(f,orient='index')\n",
        "  df.fillna(0,inplace=True)\n",
        "  g=list(df.columns)\n",
        "  y=df.label\n",
        "  x=df.drop('label',axis=1)\n",
        "  x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1234,stratify=df.label)\n",
        "  pipelines={a:make_pipeline(StandardScaler(),GaussianNB()) }\n",
        "  gnb_hyperparameters={'gaussiannb__priors':[None]}\n",
        "  hyperparameters={a:gnb_hyperparameters}\n",
        "  \n",
        "  for name,pipeline in pipelines.items():\n",
        "    model=GridSearchCV(pipeline,hyperparameters[name],cv=5)\n",
        "    model.fit(x_train,y_train)\n",
        "    fitted_models[name]=model\n",
        "    print(name)\n",
        "  return f,g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKUBQmLe4krY",
        "colab_type": "code",
        "outputId": "da994936-7d4e-432b-d9df-3745ac3d6273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "###driver code for the above functions\n",
        "fitted_models={}\n",
        "ngramlan('teluguw.txt','telugungrams.txt')\n",
        "ngramlan('bengaliW.txt','bengalingrams.txt')\n",
        "ngramlan('gujaratiW.txt','gujaratingrams.txt')\n",
        "ngramlan('hindiW.txt','hindingrams.txt')\n",
        "ngramlan('kannadaW.txt','kannadangrams.txt')\n",
        "ngramlan('malayalamW.txt','malayalamngrams.txt')\n",
        "ngramlan('maratiw.txt','maratingrams.txt')\n",
        "ngramlan('tamil.txt','tamilngrams.txt')\n",
        "\n",
        "te_en_ngrams,te_en_columns=trainingBclassifier('te_en_',enngrams,'telugungrams.txt',fitted_models,'te')\n",
        "bn_en_ngrams,bn_en_columns=trainingBclassifier('bn_en_',enngrams,'bengalingrams.txt',fitted_models,'bn')\n",
        "gu_en_ngrams,gu_en_columns=trainingBclassifier('gu_en_',enngrams,'gujaratingrams.txt',fitted_models,'gu')\n",
        "hi_en_ngrams,hi_en_columns=trainingBclassifier('hi_en_',enngrams,'hindingrams.txt',fitted_models,'hi')\n",
        "kn_en_ngrams,kn_en_columns=trainingBclassifier('kn_en_',enngrams,'kannadangrams.txt',fitted_models,'kn')\n",
        "ml_en_ngrams,ml_en_columns=trainingBclassifier('ml_en_',enngrams,'malayalamngrams.txt',fitted_models,'ml')\n",
        "ta_en_ngrams,ta_en_columns=trainingBclassifier('ta_en_',enngrams,'tamilngrams.txt',fitted_models,'ta')\n",
        "mr_en_ngrams,mr_en_columns=trainingBclassifier('mr_en_',enngrams,'maratingrams.txt',fitted_models,'mr')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "te_en_\n",
            "bn_en_\n",
            "gu_en_\n",
            "hi_en_\n",
            "kn_en_\n",
            "ml_en_\n",
            "ta_en_\n",
            "mr_en_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi1yOVu447Oq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##constructing ngrams at the sentence level\n",
        "def sentngrams(a,b,c,d):\n",
        "  t=open(a,'r')\n",
        "  w=open(b,'r')\n",
        "  z=open(c,'w')\n",
        "  y=open(d,'w')\n",
        "  langu=['en','te','kn','ta','gu','bn','hi','mr','ml']\n",
        "  engu=['en']\n",
        "  l=[]\n",
        "  for line in t:\n",
        "    list1=[]\n",
        "    l=list(w.readline().split())\n",
        "    list1=list(set(l)&set(langu))\n",
        "    if (len(list1)==2 and 'en' in list1) or list1==engu:\n",
        "      list2=[]\n",
        "      list2=list1\n",
        "      for i in list2:\n",
        "        z.write(i)\n",
        "        z.write(\"_\")\n",
        "      for word in line.split():\n",
        "        if word.isalpha():\n",
        "          list2=[]\n",
        "          list2=ngrams(5,word)\n",
        "          for i in list2:\n",
        "            z.write('\\t')\n",
        "            z.write(i)\n",
        "      for word in l:\n",
        "        y.write(word)\n",
        "        y.write('\\t')\n",
        "    y.write('\\n')    \n",
        "    z.write('\\n')\n",
        "  t.close()\n",
        "  w.close()\n",
        "  z.close()\n",
        "  y.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiwKduyXA1xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##drivercode for above function\n",
        "sentngrams('InputTraining.txt','AnnotationTraining.txt','ngramstrain.txt','ngramstrainannote.txt')\n",
        "sentngrams('InputTesting.txt','AnnotationTesting.txt','ngramstest.txt','ngramstestannote.txt')\n",
        "##making the sentence level ngrams into a dataframe for constructing classifier\n",
        "t=open('ngramstrain.txt','r')\n",
        "i=0\n",
        "ngrams1={}\n",
        "for line in t:\n",
        "    dic={}\n",
        "    l=list(line.split())\n",
        "    if len(l)>0:\n",
        "      dic['label']=l[0]\n",
        "      for j in range(1,len(l)):\n",
        "        if l[j] not in dic and l[j].isalpha():\n",
        "            dic[l[j]]=1\n",
        "        elif l[j].isalpha():\n",
        "            dic[l[j]]=dic[l[j]]+1\n",
        "    \n",
        "    \n",
        "    ngrams1[i]=dic\n",
        "    i=i+1\n",
        "t.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vp13mnJKkAC",
        "colab_type": "code",
        "outputId": "07eb4e42-f8e3-4f1a-e4c3-fe199500eb35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "##constrcuting the classifiers\n",
        "df=pd.DataFrame.from_dict(ngrams1,orient='index')\n",
        "df.fillna(0,inplace=True)\n",
        "df.label.replace('en_bn_','bn_en_',inplace=True)\n",
        "df.label.replace('en_kn_','kn_en_',inplace=True)\n",
        "df.label.replace('en_te_','te_en_',inplace=True)\n",
        "df.label.replace('en_mr_','mr_en_',inplace=True)\n",
        "df.label.replace('en_ml_','ml_en_',inplace=True)\n",
        "df.label.replace('en_gu_','gu_en_',inplace=True)\n",
        "df.label.replace('en_ta_','ta_en_',inplace=True)\n",
        "df.label.replace('en_hi_','hi_en_',inplace=True)\n",
        "y=df.label\n",
        "x=df.drop('label',axis=1)\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33,random_state=1234)\n",
        "pipelines1={\n",
        "    \n",
        "    'gnb':make_pipeline(StandardScaler(),GaussianNB()),\n",
        "    'mlp':make_pipeline(StandardScaler(),MLPClassifier(random_state=123)),\n",
        "    'l2':make_pipeline(StandardScaler(),LogisticRegression(penalty='l2',random_state=123))\n",
        "}\n",
        "gnb_hyperparameters={'gaussiannb__priors':[None]}\n",
        "mlp_hyperparameters={'mlpclassifier__activation':['relu'],'mlpclassifier__max_iter':[200]}\n",
        "l2_hyperparameters={'logisticregression__max_iter':[1000]}\n",
        "hyperparameters={'gnb':gnb_hyperparameters,'mlp':mlp_hyperparameters,'l2':l2_hyperparameters,}\n",
        "fitted_models1={}\n",
        "for name,pipeline in pipelines1.items():\n",
        "    model=GridSearchCV(pipeline,hyperparameters[name],cv=5)\n",
        "    model.fit(x_train,y_train)\n",
        "    fitted_models1[name]=model\n",
        "    print(name)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gnb\n",
            "mlp\n",
            "l2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5IEHdhSmTWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##predict at word level\n",
        "def predict(pred1,file):\n",
        "  preda=[]\n",
        "  t=open(file,'w')  \n",
        "  j=0\n",
        "  lane=0\n",
        "  for i in pred1:\n",
        "    word_list=[]\n",
        "    k=0\n",
        "    d=x_test.index.values[j]\n",
        "    line = linecache.getline('ngramstrain.txt',d+1)\n",
        "    line1=linecache.getline('ngramstrainannote.txt',d+1)\n",
        "    str2=''\n",
        "    for word in line.split():\n",
        "      if len(word)==1:\n",
        "        str2=str2+word\n",
        "      elif len(str2)>0:\n",
        "        word_list.append(str2)\n",
        "        str2=''\n",
        "    if i!='en_':\n",
        "     \n",
        "      list2=[]\n",
        "     \n",
        "      \n",
        "      for j in range(len(word_list)):\n",
        "        dic={}\n",
        "        we=col[i]\n",
        "        for some in we:\n",
        "          dic[some]=0\n",
        "        list1=[]\n",
        "        list1=ngrams(5,word[j])\n",
        "        for word1 in list1:\n",
        "          if word1 in dic:\n",
        "            dic[word1]=dic[word1]+1\n",
        "        df=pd.DataFrame.from_dict(dic,orient='index')\n",
        "        df=df.transpose()\n",
        "        df=df.drop('label',axis=1)\n",
        "        df.append(dic,ignore_index=True)\n",
        "        pred=fitted_models[i].predict(df)\n",
        "        t.write('\\n')\n",
        "        t.write(word[j])\n",
        "        t.write('\\t')\n",
        "        t.write(line1[j])\n",
        "        t.write('\\t')\n",
        "        t.write(pred[0])\n",
        "        \n",
        "        preda.append(pred)\n",
        "    else:\n",
        "      ##all will be english\n",
        "      for word in word_list:\n",
        "        t.write('\\n')\n",
        "        t.write(word)\n",
        "        t.write('\\t')\n",
        "        t.write('en')\n",
        "        preda.append('en')\n",
        "\n",
        "    j=j+1\n",
        "    print(j)\n",
        "    t.write('\\n')\n",
        "  t.close()  \n",
        "  return preda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4zl8knOMIJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##driver code for prediction and savving into a text file\n",
        "o={'te_en_':te_en_ngrams,'bn_en_':bn_en_ngrams,'kn_en_':kn_en_ngrams,'hi_en_':hi_en_ngrams,'gu_en':gu_en_ngrams,'ml_en_':ml_en_ngrams,'mr_en_':mr_en_ngrams,\n",
        "   'ta_en_':ta_en_ngrams\n",
        "}\n",
        "col={'te_en_':te_en_columns,'bn_en_':bn_en_columns,'kn_en_':kn_en_columns,'hi_en_':hi_en_columns,'gu_en_':gu_en_columns,'ml_en_':ml_en_columns,'mr_en_':mr_en_columns,\n",
        "   'ta_en_':ta_en_columns}\n",
        "\n",
        "\n",
        "for name,model in fitted_models1.items():\n",
        "  pred=model.predict(x_test)\n",
        "  if name=='gnb':\n",
        "    preda=predict(pred,'predicted_words_gnb.txt')\n",
        "  elif name=='mlp':\n",
        "    preda=predict(pred,'predicted_words_mlp.txt')\n",
        "  elif name=='l2':\n",
        "     preda=predict(pred,'predicted_words_l2.txt')\n",
        "  print(f1_score(y_test,pred,average='weighted'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9XburRRqgWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}